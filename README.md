# ğŸ•·ï¸ Distributed Web Scraping System

âš¡ **EscalÃ¡vel. ObservÃ¡vel. Resiliente.**
Um sistema de **Web Scraping distribuÃ­do** com **monitoramento em tempo real**, usando **Celery + Redis + Prometheus + Grafana**.

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11-blue?style=for-the-badge&logo=python" />
  <img src="https://img.shields.io/badge/docker-compose-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white" />
  <img src="https://img.shields.io/badge/grafana-%23F46800.svg?style=for-the-badge&logo=grafana&logoColor=white" />
  <img src="https://img.shields.io/badge/prometheus-%23E6522C.svg?style=for-the-badge&logo=prometheus&logoColor=white" />
  <img src="https://img.shields.io/badge/redis-%23DD0031.svg?style=for-the-badge&logo=redis&logoColor=white" />
</p>  

---

## ğŸš€ VisÃ£o Geral

Este projeto resolve o problema de **raspagem massiva de pÃ¡ginas web** de forma:

* ğŸ”¥ **DistribuÃ­da** â€“ mÃºltiplos workers processando jobs em paralelo.
* ğŸ§  **Inteligente** â€“ filas controladas via **Redis**.
* ğŸ“Š **Monitorada** â€“ mÃ©tricas em tempo real com **Prometheus + Grafana**.
* ğŸ’ª **Resiliente** â€“ tolera falhas de scraping e reprocessa URLs automaticamente.

---

## ğŸ—ï¸ Arquitetura

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Cliente   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                        â”‚
                  HTTP Requests
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                 â”‚    API     â”‚  â† expÃµe mÃ©tricas (/metrics)
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                        â”‚ Envia jobs
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                 â”‚   Redis    â”‚  â† fila de scraping
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Worker 1      â”‚          â”‚   Worker N     â”‚
 â”‚ Celery + Scraper â”‚   ...    â”‚ Celery + Scraperâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                           â”‚
     Jobs concluÃ­dos            Jobs concluÃ­dos
          â”‚                           â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
     â”‚ Databaseâ”‚                 â”‚ Storage â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Monitoramento

* **Prometheus (9090)** â†’ coleta mÃ©tricas da API e workers.
* **Grafana (3000)** â†’ dashboards com insights em tempo real.
* **Redis (6379)** â†’ monitoramento de filas e jobs.

### ğŸ”‘ MÃ©tricas disponÃ­veis

* `api_requests_total` â†’ total de requisiÃ§Ãµes.
* `queue_jobs_total` â†’ jobs enfileirados.
* `scrape_success_total` â†’ scrapes bem-sucedidos.
* `scrape_fail_total` â†’ falhas no scraping.
* `api_request_latency` â†’ latÃªncia das requisiÃ§Ãµes.
* `process_resident_memory_bytes` â†’ memÃ³ria usada.

---

## âš™ï¸ Como Rodar

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/distributed-web-scraping.git](https://github.com/Zezoca29/Sistema-Web-Scraping-Docker.git
cd distributed-web-scraping
```

### 2. Suba os containers

```bash
docker compose up -d --build
```

### 3. Acesse os serviÃ§os

* API â†’ [http://localhost:8000](http://localhost:8000)
* Prometheus â†’ [http://localhost:9090](http://localhost:9090)
* Grafana â†’ [http://localhost:3000](http://localhost:3000) (login: `admin/admin`)

---

## ğŸ“ˆ Dashboard Grafana

<p align="center">
  <img src="https://grafana.com/static/img/logos/grafana/grafana.png" width="120"/>
</p>  

O Grafana jÃ¡ vem configurado com o dashboard **â€œDistributed Web Scraping System Monitorâ€**, incluindo:

* ğŸ“¡ TrÃ¡fego da API (requisiÃ§Ãµes/s, latÃªncia).
* ğŸ•·ï¸ Jobs processados (sucesso/falha).
* ğŸ”´ Status das filas Redis.
* ğŸ–¥ï¸ Uso de memÃ³ria/CPU dos workers.

---

## ğŸ¤– Scripts Ãšteis

* **Abrir dashboards de monitoramento**

```powershell
.\open_monitoring.ps1
```

* **Rodar health check**

```powershell
.\monitor_system.ps1
```

* **Gerar mÃ©tricas de teste**

```powershell
.\test_working.ps1
```

---

## ğŸ”® PrÃ³ximos Passos

* [ ] Configurar **alertas no Grafana** (falhas > X%).
* [ ] Adicionar mÃ©tricas especÃ­ficas do negÃ³cio (tempo mÃ©dio por domÃ­nio).
* [ ] Monitoramento avanÃ§ado de filas do Redis.
* [ ] IntegraÃ§Ã£o com **ELK Stack** para logs centralizados.

---

## ğŸ† Por que esse projeto Ã© insano?

* âœ… Arquitetura **real de produÃ§Ã£o**.
* âœ… **Observabilidade completa** (mÃ©tricas, dashboards, health checks).
* âœ… DemonstraÃ§Ã£o de **skills avanÃ§adas**: DevOps, Python, DistribuiÃ§Ã£o, Monitoramento.

---

ğŸ”¥ **Distributed Web Scraping System** Ã© mais que um scraper â€” Ã© um **exemplo de arquitetura escalÃ¡vel e profissional**, pronto para ser usado em projetos reais.

---

